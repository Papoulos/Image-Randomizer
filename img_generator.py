import random
import requests
import json
import time
import os
import psutil
import subprocess
import datetime
import argparse
import base64

# Import configuration and prompts
from config import (
    SAVE_DIR, OL_models, Prompt_list, OLLAMA_PORT, MAX_RETRIES, TIMEOUT,
    SDXL_CONFIG, FLUX_CONFIG, COMFYUI_URL, COMFYUI_OUTPUT_DIR, IMAGE_TIMEOUT
)
from prompts import generate_random_prompt

# =======================
# Process Management
# =======================

def is_process_running(name):
    for proc in psutil.process_iter(attrs=['pid', 'name']):
        if name.lower() in proc.info['name'].lower():
            return True
    return False

def is_server_alive(url):
    try:
        response = requests.get(url, timeout=5)
        return response.status_code == 200
    except requests.RequestException:
        return False

def kill_ollama():
    for proc in psutil.process_iter(attrs=['pid', 'name']):
        if "ollama" in proc.info['name'].lower():
            proc.kill()

def start_ollama():
    if is_server_alive(f"http://127.0.0.1:{OLLAMA_PORT}"):
        return True

    if not is_process_running("ollama"):
        print("üöÄ D√©marrage d'Ollama...")
        subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        time.sleep(5)

    for _ in range(MAX_RETRIES):
        if is_server_alive(f"http://127.0.0.1:{OLLAMA_PORT}"):
            print("‚úÖ Ollama est en ligne !")
            return True
        time.sleep(2)

    print("‚ùå √âchec du d√©marrage d'Ollama.")
    return False

# =======================
# Prompt & LoRA Generation (Ollama)
# =======================

def call_ollama(prompt_template, input_data):
    """Generic function to call Ollama LLM."""
    if not start_ollama(): return None
    
    command = ["ollama", "run", OL_models[0]]
    if isinstance(input_data, tuple):
        full_prompt = prompt_template.format(*input_data)
    else:
        full_prompt = prompt_template.format(input_data)

    try:
        process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding="utf-8")
        output, error = process.communicate(input=full_prompt, timeout=60)
        if process.returncode == 0:
            return output.strip()
        else:
            print(f"‚ùå Erreur Ollama : {error}")
            return None
    except subprocess.TimeoutExpired:
        print("‚ö†Ô∏è Timeout: Ollama a mis trop de temps √† r√©pondre.")
        process.kill()
        return None

def generate_prompt_only(base_prompt):
    """Generates a detailed prompt without LoRA syntax."""
    prompt_template = """Vous √™tes un expert en cr√©ation de prompts pour les IA g√©n√©ratives d'images comme Stable Diffusion XL. Votre objectif est de transformer une description de sc√®ne simple en un prompt riche, structur√© et efficace. Le prompt doit √™tre clair, visuellement √©vocateur et optimis√© pour une g√©n√©ration d'image de haute qualit√©.

Vous recevrez une description de base contenant des √©l√©ments cl√©s (sujet, action, lieu, etc.).

Suivez imp√©rativement ces r√®gles pour cr√©er le prompt final :

1.  **Structure du Prompt :**
    *   Le prompt final doit √™tre un bloc de texte unique, **en anglais**, compos√© de segments descriptifs s√©par√©s par des virgules. Ne formez pas une seule longue phrase grammaticale.
    *   Organisez le prompt dans cet ordre logique :
        1.  **Sujet et Action :** Commencez par le sujet principal et son action.
        2.  **Description D√©taill√©e :** D√©crivez les d√©tails importants du sujet (v√™tements, apparence, expression).
        3.  **D√©cor et Environnement :** D√©crivez la sc√®ne, l'arri√®re-plan et les √©l√©ments contextuels.
        4.  **Ambiance et √âclairage :** Ajoutez des mots-cl√©s pour l'ambiance (mood), la lumi√®re (lighting) et la composition.
        5.  **Style Artistique :** Terminez **toujours** par le style (ex: `photorealistic`, `oil painting`, `anime style`, `cinematic`).

2.  **Mise en Emphase :**
    *   Identifiez le **sujet principal** et/ou l'**action cl√©**. Mettez-le(s) en emphase en l'entourant de parenth√®ses avec un poids entre `1.4` et `1.6`. N'appliquez ce poids qu'√† 1 ou 2 √©l√©ments maximum.
    *   Exemple : `(a beautiful warrior princess:1.5), (fighting a dragon:1.4)`.

3.  **Enrichissement Cr√©atif :**
    *   Injectez **un ou deux d√©tails cr√©atifs et coh√©rents** qui ne sont pas dans la description initiale. Ces ajouts doivent enrichir la sc√®ne (ex: un d√©tail sur la m√©t√©o, un objet en arri√®re-plan, une texture particuli√®re).

4.  **Contraintes de Qualit√© et Format :**
    *   Soyez descriptif mais **concis**. Visez une longueur totale de **40 √† 80 mots**.
    *   Utilisez des mots-cl√©s forts et visuels. √âvitez les descriptions vagues.
    *   Le prompt doit √™tre une seule cha√Æne de caract√®res, sans retour √† la ligne.
    *   Ne produisez **que** le prompt final. N'ajoutez aucune explication, introduction, commentaire ou excuse.

Traitez la description suivante :
{}"""
    return call_ollama(prompt_template, base_prompt)

def select_lora_with_llm(prompt, config):
    """Selects the most appropriate LoRA for a given prompt."""
    all_loras = list(set(lora for loras in config["lora_themes"].values() for lora in loras))
    if not all_loras: return None

    prompt_template = 'From the following list, which LoRA is most thematically appropriate for the prompt below? Respond with ONLY the name of the LoRA.\n\nLoRA List: {1}\n\nPrompt: "{0}"'
    return call_ollama(prompt_template, (prompt, all_loras))

# =======================
# ComfyUI API Interaction
# =======================

def update_workflow(workflow_data, config, prompt, lora_name):
    """Robustly updates the ComfyUI API workflow dictionary."""
    prompt_node_id = config["prompt_node_id"]
    lora_node_id = config["lora_node_id"]

    # Update the prompt text in the specified node
    if prompt_node_id in workflow_data:
        workflow_data[prompt_node_id]["inputs"]["text"] = prompt
        print(f"‚úÖ Prompt inject√© dans le noeud {prompt_node_id}.")
    else:
        print(f"‚ùå Erreur: Noeud de prompt ID '{prompt_node_id}' non trouv√© dans le workflow.")

    # Update the LoRA name in the specified node, if a LoRA is provided
    if lora_name and lora_node_id in workflow_data:
        workflow_data[lora_node_id]["inputs"]["lora_name"] = lora_name
        print(f"‚úÖ LoRA '{lora_name}' inject√© dans le noeud {lora_node_id}.")
    elif lora_name:
        # This case handles when a lora_name is available but the node ID is not found
        print(f"‚ùå Erreur: Noeud de LoRA ID '{lora_node_id}' non trouv√© dans le workflow.")

    return workflow_data

def queue_prompt(workflow):
    """Queues a prompt on the ComfyUI server."""
    payload = {"prompt": workflow}
    try:
        response = requests.post(f"{COMFYUI_URL}/prompt", json=payload)
        response.raise_for_status()
        return response.json()['prompt_id']
    except requests.RequestException as e:
        print(f"‚ùå Erreur lors de l'envoi √† ComfyUI: {e}")
        return None

def get_image(prompt_id):
    """Polls the ComfyUI history and retrieves the generated image."""
    print("‚è≥ En attente de la g√©n√©ration de l'image par ComfyUI...")
    # Poll for IMAGE_TIMEOUT seconds max
    for _ in range(IMAGE_TIMEOUT // 2):
        try:
            res = requests.get(f"{COMFYUI_URL}/history/{prompt_id}")
            res.raise_for_status()
            history = res.json()
            if prompt_id in history and history[prompt_id].get('outputs'):
                outputs = history[prompt_id]['outputs']
                for node_id in outputs:
                    if 'images' in outputs[node_id]:
                        img_info = outputs[node_id]['images'][0]
                        img_path = os.path.join(COMFYUI_OUTPUT_DIR, img_info.get('subfolder', ''), img_info['filename'])
                        print(f"‚úÖ Image trouv√©e : {img_path}")
                        if os.path.exists(img_path):
                            with open(img_path, 'rb') as f:
                                return f.read()
                return None # Still processing
            time.sleep(2)
        except requests.RequestException as e:
            print(f"‚ùå Erreur de connexion √† ComfyUI : {e}")
            return None
    print("‚ùå Timeout: L'image n'a pas √©t√© g√©n√©r√©e √† temps.")
    return None

# =======================
# File Saving
# =======================

def save_image(image_data, filename_prefix):
    """Saves image data to the specified save directory."""
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{filename_prefix}_{timestamp}.png"
    image_path = os.path.join(SAVE_DIR, filename)
    try:
        with open(image_path, 'wb') as f:
            f.write(image_data)
        print(f"‚úÖ Image sauvegard√©e √† : {image_path}")
    except Exception as e:
        print(f"‚ùå Erreur de sauvegarde: {e}")

# =======================
# Main Generation Loop
# =======================

def main_generation_loop(config, num_iterations):
    """The main unified generation loop."""
    for i in range(1, num_iterations + 1):
        print(f"\n--- It√©ration {i}/{num_iterations} ---")

        # 1. Load workflow template
        with open(config['workflow_file'], 'r', encoding='utf-8-sig') as f:
            workflow_wrapper = json.load(f)

        # The API format wraps the workflow in a "prompt" key. We need to extract it.
        workflow = workflow_wrapper.get("prompt")
        if not workflow:
            print(f"‚ùå Erreur: Le fichier workflow '{config['workflow_file']}' ne semble pas √™tre au format API correct (cl√© 'prompt' manquante).")
            continue

        # 2. Generate prompt
        base_prompt, _ = generate_random_prompt()
        prompt = generate_prompt_only(base_prompt)
        if not prompt:
            print("‚ö†Ô∏è Impossible de g√©n√©rer un prompt, passage √† l'it√©ration suivante.")
            continue
        print(f"üìù Prompt: {prompt[:100]}...")

        # 3. Select LoRA
        lora = select_lora_with_llm(prompt, config)
        if not lora:
            print("‚ö†Ô∏è Impossible de s√©lectionner un LoRA.")
        else:
            print(f"üé® LoRA: {lora}")

        # 4. Update workflow and queue for generation
        updated_workflow = update_workflow(workflow, config, prompt, lora)
        prompt_id = queue_prompt(updated_workflow)
        
        # 5. Get and save the image
        if prompt_id:
            image_data = get_image(prompt_id)
            if image_data:
                save_image(image_data, "generated")
        
        time.sleep(5)

# =======================
# Entry Point
# =======================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="G√©n√©rateur d'images unifi√© via ComfyUI.")
    parser.add_argument("--flux", action="store_true", help="Utiliser le workflow Flux au lieu de SDXL.")
    parser.add_argument("--iterations", type=int, default=10, help="Nombre d'it√©rations de g√©n√©ration.")
    args = parser.parse_args()

    active_config = FLUX_CONFIG if args.flux else SDXL_CONFIG
    model_type = "Flux" if args.flux else "SDXL"

    print(f"üöÄ D√©marrage du g√©n√©rateur d'images en mode {model_type} via ComfyUI.")

    main_generation_loop(active_config, args.iterations)
